{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fdf411",
   "metadata": {},
   "source": [
    "### Beautiful Soup: Build a Web Scraper With Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb2155",
   "metadata": {},
   "source": [
    "* #### Inspect the HTML structure of your target site with your browser’s developer tools\n",
    "\n",
    "Before you write any Python code, you need to get to know the website that you want to scrape. That should be your first step for any web scraping project you want to tackle. You’ll need to understand the site structure to extract the information that’s relevant for you. Start by opening the site you want to scrape with your favorite browser.\n",
    "* #### Decipher data encoded in URLs\n",
    "\n",
    "A programmer can encode a lot of information in a URL. Your web scraping journey will be much easier if you first become familiar with how URLs work and what they’re made of. For example, you might find yourself on a details page that has the following URL:\n",
    "\n",
    "https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\n",
    "You can deconstruct the above URL into two main parts:\n",
    "\n",
    "1. The base URL represents the path to the search functionality of the website. In the example above, the base URL is https://realpython.github.io/fake-jobs/.\n",
    "2. The specific site location that ends with .html is the path to the job description’s unique resource.\n",
    "Any job posted on this website will use the same base URL. However, the unique resources’ location will be different depending on what specific job posting you’re viewing.\n",
    "\n",
    "URLs can hold more information than just the location of a file. Some websites use query parameters to encode values that you submit when performing a search. You can think of them as query strings that you send to the database to retrieve specific records.\n",
    "* #### Use ```requests``` and ```Beautiful Soup``` for scraping and parsing data from the Web\n",
    "\n",
    "Beautiful Soup is a Python library for parsing structured data. It allows you to interact with HTML in a similar way to how you interact with a web page using developer tools. The library exposes a couple of intuitive functions you can use to explore the HTML you received. \n",
    "* #### Step through a web scraping pipeline from start to finish\n",
    "* #### Build a script that fetches job offers from the Web and displays relevant information in your console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff8dd47",
   "metadata": {},
   "source": [
    "#### Challenges of Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff32f7",
   "metadata": {},
   "source": [
    "* <b> Variety:</b> Every website is different. While you’ll encounter general structures that repeat themselves, each website is unique and will need personal treatment if you want to extract the relevant information.\n",
    "\n",
    "* <b> Durability:</b> Websites constantly change. Say you’ve built a shiny new web scraper that automatically cherry-picks what you want from your resource of interest. The first time you run your script, it works flawlessly. But when you run the same script only a short while later, you run into a discouraging and lengthy stack of tracebacks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e9d08",
   "metadata": {},
   "source": [
    "### An Alternative to Web Scraping: APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dabf1c2",
   "metadata": {},
   "source": [
    "Some website providers offer application programming interfaces (APIs) that allow you to access their data in a predefined manner. With APIs, you can avoid parsing HTML. Instead, you can access the data directly using formats like JSON and XML. HTML is primarily a way to present content to users visually.\n",
    "\n",
    "When you use an API, the process is generally more stable than gathering the data through web scraping. That’s because developers create APIs to be consumed by programs rather than by human eyes.\n",
    "\n",
    "The front-end presentation of a site might change often, but such a change in the website’s design doesn’t affect its API structure. The structure of an API is usually more permanent, which means it’s a more reliable source of the site’s data.\n",
    "\n",
    "However, APIs can change as well. The challenges of both variety and durability apply to APIs just as they do to websites. Additionally, it’s much harder to inspect the structure of an API by yourself if the provided documentation lacks quality.\n",
    "\n",
    "The approach and tools you need to gather information using APIs are outside the scope of this tutorial. To learn more about it, check out API Integration in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad81482",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from_url = 'https://www.estate.am/բնակարաններ-երևանում-s3990854?page='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_apartment_urls = list()\n",
    "count = 0\n",
    "for pages in range(1,10000):\n",
    "    URL = download_from_url + f'{pages}'\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    house_url = soup.find_all('a', class_='btn-details', title=\"ավելին\")\n",
    "    if len(house_url) == 0:\n",
    "        break\n",
    "    for house_href in house_url:\n",
    "        complete_house_urls = 'https://www.estate.am/en' + house_href.get('href')\n",
    "        count+=1\n",
    "        all_apartment_urls.append([count, complete_house_urls])\n",
    "print(\"DB rows: \", len(all_apartment_urls))\n",
    "urls_to_csv = pd.DataFrame(all_apartment_urls, columns=['row', 'url'])\n",
    "urls_to_csv.to_csv('urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76964101",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "appartment_db = list()\n",
    "count = 0\n",
    "for apartment in urls_to_csv.url.tolist()[:100]:\n",
    "    page = requests.get(apartment)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    time.sleep(1.5)\n",
    "    appartment_db.append(\n",
    "    {\n",
    "        'addr': soup.find('strong', class_='addr').text,\n",
    "        'ruler': soup.find('span', class_='ruler').text,\n",
    "        'price': re.sub('\\s+', '', soup.find('div', class_='price-w').text)\n",
    "        \n",
    "    })\n",
    "    count+=1\n",
    "    print('Observation: ', count)\n",
    "apartments = pd.DataFrame(appartment_db)\n",
    "apartments.to_csv('apartments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f86cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
