{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Analysis And Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customer Churn**\n",
    "It is when an existing customer, user, subscriber, or any kind of return client stops doing business or ends the relationship with a company.\n",
    "\n",
    "**Types of Customer Churn**\n",
    "\n",
    "- Contractual Churn : When a customer is under a contract for a service and decides to cancel the service e.g. Cable TV, SaaS.\n",
    "- Voluntary Churn : When a user voluntarily cancels a service e.g. Cellular connection.\n",
    "- Non-Contractual Churn : When a customer is not under a contract for a service and decides to cancel the service e.g. Consumer - Loyalty in retail stores.\n",
    "- Involuntary Churn : When a churn occurs without any request of the customer e.g. Credit card expiration.\n",
    "\n",
    "**Reasons for Voluntary Churn**\n",
    "- Lack of usage\n",
    "- Poor service\n",
    "- Better price\n",
    "\n",
    "**Data Attributes and Labels** \n",
    "<br>\n",
    "The initial ingredient for building any predictive pipeline is data. For churn specifically, historical data is captured and stored in a data warehouse, depending on the application domain. The process of churn definition and establishing data hooks to capture relevant events is highly iterative. It is very important to keep this in mind as the initial churn definition, with its associated data hooks, may not be applicable or relevant anymore as a product or a service matures. That’s why it’s essential for data scientists to not only monitor the performance of the predictive pipeline over time but also to pay close attention to the alignment of churn definition with the product’s changes as they might affect who the churners are.\n",
    "\n",
    "The specific attributes used in a churn model are highly domain dependent. However, broadly speaking, the most common attributes capture user behavior with regards to engagement level with a product or service. This can be thought of as the number of times that a user logs into her/his account in a week or the amount of time that a user spends on a portal. In short, frequency and intensity of usage/engagement are among the strongest signals to predict churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the data for analysis [here.](https://www.kaggle.com/barun2104/telecom-churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score,precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "data = pd.read_csv('data/telecom_churn.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Description**\n",
    "\n",
    "- Churn - 1 if customer cancelled service, 0 if not\n",
    "- AccountWeeks - number of weeks customer has had active account\n",
    "- ContractRenewal - 1 if customer recently renewed contract, 0 if not\n",
    "- DataPlan - 1 if customer has data plan, 0 if not\n",
    "- DataUsage - gigabytes of monthly data usage\n",
    "- CustServCalls - number of calls into customer service\n",
    "- DayMins - average daytime minutes per month\n",
    "- DayCalls - average number of daytime calls\n",
    "- MonthlyCharge - average monthly bill\n",
    "- OverageFee - largest overage fee in last 12 months\n",
    "- RoamMins - average number of roaming minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General information about dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ 'ContractRenewal', 'DataPlan' ]\n",
    "\n",
    "for col in cols:\n",
    "    data[col] = data[col].astype('O')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, the first thing we're going to do is explore our dataset. We have telecom customer data where each row represents a unique customer. In addition, we have a churn status, with values 1 if the customer has churned, and 0 if we have an active customer. However, we don't always have these labels, and sometimes we need to create them from scratch based on our prior business knowledge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_continuous(dataset, var_name):\n",
    "    sns.displot(dataset[var_name])\n",
    "    plt.axvline(dataset[var_name].mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "    plt.axvline(dataset[var_name].median(), color='r', linewidth=1)\n",
    "    plt.title(f'Distribution of variable \"{var_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_objects(dataset, var_name):\n",
    "    sns.countplot(dataset[var_name])\n",
    "    plt.title(f'Distribution of variable \"{var_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major problem with our data is that we have an unbalanced target. As we can see in the graph below, our dataset has more active customers than churned ones. Later we will see some techniques that will help us solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_objects(data, 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous(data, 'AccountWeeks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous(data, 'DataUsage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous(data, 'CustServCalls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous(data, 'DayMins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous(data, 'DayCalls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous(data, 'MonthlyCharge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous(data, 'OverageFee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous(data, 'RoamMins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_objects(data, 'ContractRenewal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_objects(data, 'DataPlan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find relations between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring our variables separately, it is now turn to find the relationship between two or more variables. And remember that churn is our target variable and we need to first find some issues that may cause it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['AccountWeeks', 'DataUsage', 'DayMins','CustServCalls', 'DayCalls','MonthlyCharge', 'OverageFee', 'RoamMins' ]\n",
    "sns.pairplot(data, vars = cols, hue = 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "s = sns.heatmap(data.corr(),\n",
    "               annot = True, \n",
    "               cmap = 'RdBu',\n",
    "               vmin = -1, \n",
    "               vmax = 1)\n",
    "\n",
    "s.set_yticklabels(s.get_yticklabels(), rotation = 0, fontsize = 12)\n",
    "s.set_xticklabels(s.get_xticklabels(), rotation = 90, fontsize = 12)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dive into those two graphs above. We see that there is no obvious relationship between churn and other variables. However, there are other interesting discoveries worth discussing. For example, Monthly Charge and Data Usage are highly correlated, as are Monthly Charge and Daily Calls. \n",
    "\n",
    "Next we will look at the interaction of categorical variables with our target variable. The following heatmap shows that customers who have a data plan are less likely to leave. In the next heatmap, it's clear that customers who haven't renewed their contracts are more likely to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.crosstab(data['Churn'], data['DataPlan']).apply(lambda r: r/r.sum(), axis=1)\n",
    "\n",
    "plt.figure(figsize = (5, 3))\n",
    "s = sns.heatmap(cross,\n",
    "               annot = True, \n",
    "               cmap = 'RdBu',\n",
    "               vmin = 0, \n",
    "               vmax = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.crosstab(data['Churn'], data['ContractRenewal']).apply(lambda r: r/r.sum(), axis=1)\n",
    "\n",
    "plt.figure(figsize = (5, 3))\n",
    "s = sns.heatmap(cross,\n",
    "               annot = True, \n",
    "               cmap = 'RdBu',\n",
    "               vmin = 0, \n",
    "               vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, there isn't much of a relationship between churn and customer service calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.crosstab(data['Churn'], data['CustServCalls']).apply(lambda r: r/r.sum(), axis=1)\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "s = sns.heatmap(cross,\n",
    "               annot = True, \n",
    "               cmap = 'RdBu',\n",
    "               vmin = 0, \n",
    "               vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.Churn.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we have unbalanced data, which means that single-label observations dominate. Here we can see that only 17% of the data is marked as \"churned\". The modeling result may be either too good to be believed or we may get incorrectly generalized model. So what can be done? There are several techniques that we can try on and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: without balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from modeling the original dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['Churn'].astype(int)\n",
    "X = data.drop(columns='Churn')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=['ContractRenewal', 'DataPlan'])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(X,target,test_size=0.25,random_state=42)\n",
    "print(f\"Number of observations \\n Train set: {len(train_x)}\\n Test set: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try 3 different models: Logistic Regression, k Nearest Neighbor, Random Forest classifier. We're not going to dive into hyperparameter tuning, instead we're just trying to experiment with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic = LogisticRegression(random_state=42)\n",
    "Knn = KNeighborsClassifier(n_neighbors=5)\n",
    "RFC = RandomForestClassifier(n_estimators=100, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(cls_list):\n",
    "    indx = [ 'Acc_train', 'Acc_test', 'Recall_train', 'Recall_test', 'Precision_train', 'Precision_test']\n",
    "    \n",
    "    results = pd.DataFrame(index=indx, columns=['Logistic', 'Knn', 'RFC'])\n",
    "    \n",
    "    for i, cls in enumerate(cls_list):\n",
    "        cls.fit(train_x,train_y)\n",
    "        test_y_new = cls.predict(test_x)    \n",
    "        train_y_new = cls.predict(train_x)\n",
    "        acc_train = accuracy_score(train_y,train_y_new)\n",
    "        acc_test = accuracy_score(test_y,test_y_new)\n",
    "        rec_train = recall_score(train_y,train_y_new)\n",
    "        rec_test = recall_score(test_y,test_y_new)\n",
    "        pr_train = precision_score(train_y,train_y_new)\n",
    "        pr_test = precision_score(test_y,test_y_new)\n",
    "        name = results.columns[i]\n",
    "        results[name] = [acc_train, acc_test, rec_train, rec_test, pr_train, pr_test]\n",
    "    return results  \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [ Logistic, Knn, RFC]\n",
    "res = results(classifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "s = sns.heatmap(res,\n",
    "               annot = True, \n",
    "               cmap = 'RdBu',\n",
    "               vmin = 0, \n",
    "               vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously we have an overfiting problem. Good results on train dataset, but recall and accuracy rates on test set show real problems with models. The small rate of recall is indicator of many false negatives, and precision rate shows that we have problem with false positives as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2: Down-sample Majority Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way to solve the problem of unbalanced data is to reduce the data. This means that we will remove observations from the majority class. The main problem with this method is that we can lose too much data and end up with a bad model with poor generalization.Let's try the technique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority = data[data.Churn==0]\n",
    "minority = data[data.Churn==1]\n",
    " \n",
    "\n",
    "downsampled = resample(majority, \n",
    "                       replace=False,    \n",
    "                       n_samples=483,     \n",
    "                       random_state=42) \n",
    " \n",
    "\n",
    "downsampled = pd.concat([downsampled, minority])\n",
    " \n",
    "downsampled['Churn'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = downsampled['Churn'].astype(int)\n",
    "X = downsampled.drop(columns='Churn')\n",
    "X = pd.get_dummies(X, columns=['ContractRenewal', 'DataPlan'])\n",
    "\n",
    "train_x,test_x,train_y,test_y = train_test_split(X,target,test_size=0.25,random_state=42)\n",
    "\n",
    "print(f\"Number of observations \\n Train set: {len(train_x)}\\n Test set: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = results(classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "s = sns.heatmap(res,\n",
    "               annot = True, \n",
    "               cmap = 'RdBu',\n",
    "               vmin = 0, \n",
    "               vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we solved the problem of overfitting, because the accuracy of the test and training samples does not differ too much, moreover, the recall and precision are closer to each other. However, we now have a generalization problem! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3: Up-sample Minority Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are moving to next technique: upsampling the minority class. This time we will resample the minority class so that it equals the majority. **Think about the problem this method might cause?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled = resample(minority, \n",
    "                     replace=True,     \n",
    "                     n_samples=2850,    \n",
    "                     random_state=42) \n",
    " \n",
    "\n",
    "upsampled = pd.concat([majority,upsampled])\n",
    "print(\"Duplicates: \", upsampled.duplicated().sum())\n",
    "upsampled['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = upsampled['Churn'].astype(int)\n",
    "X = upsampled.drop(columns='Churn')\n",
    "X = pd.get_dummies(X, columns=['ContractRenewal', 'DataPlan'])\n",
    "\n",
    "train_x,test_x,train_y,test_y = train_test_split(X,target,test_size=0.25,random_state=42)\n",
    "\n",
    "print(f\"Number of observations \\n Train set: {len(train_x)}\\n Test set: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = results(classifiers)\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "s = sns.heatmap(res,\n",
    "               annot = True, \n",
    "               cmap = 'RdBu',\n",
    "               vmin = 0, \n",
    "               vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, looks like we have a nearly perfect picture for a random forest classifier. But wait! What about data leakage? \n",
    "\n",
    "What actually happened, we made thousands of copies of the same observations, and then we split up into training and testing samples. As a result, we may have exactely same observations in training and testing sets, which means that we can get better results on the test dataset, but it's not convenient for real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 4: Upsample Minority Only On Train Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, in the last experiment, we will first split the data into train and test samples,then upsample data. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['Churn'].astype(int)\n",
    "X = data.drop(columns='Churn')\n",
    "X = pd.get_dummies(X, columns=['ContractRenewal', 'DataPlan'])\n",
    "\n",
    "train_x,test_x,train_y,test_y = train_test_split(X,target,test_size=0.25,random_state=42)\n",
    "\n",
    "print(f\"Number of observations \\n Train set: {len(train_x)}\\n Test set: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_x, train_y],  axis = 1)\n",
    "majority = train[train.Churn==0]\n",
    "minority = train[train.Churn==1]\n",
    "majority.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled2 = resample(minority, \n",
    "                     replace=True,     \n",
    "                     n_samples=2141,    \n",
    "                     random_state=42) \n",
    " \n",
    "\n",
    "upsampled2 = pd.concat([majority,upsampled2])\n",
    "\n",
    "upsampled2['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = upsampled2['Churn'].astype(int)\n",
    "train_x = upsampled2.drop(columns='Churn')\n",
    "\n",
    "print(f\"Number of observations \\n Train set: {len(train_x)}\\n Test set: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = results(classifiers)\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "s = sns.heatmap(res,\n",
    "               annot = True, \n",
    "               cmap = 'RdBu',\n",
    "               vmin = 0, \n",
    "               vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like what we want! Logistic regression has a high rate of recall, but lower precision, we cannot consider this as our final model. Knn has problems with both low recall and low precision. In the case of a Random Forest, we have lower recall than in Logistic regression, however we've got higher precision and overall accuracy than other models. So we will go with Random Forest classifier.\n",
    "\n",
    "What's next? \n",
    "- Try another model and compare the results.\n",
    "- Fine-tune the models we already have.\n",
    "- Create meaningful new features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
